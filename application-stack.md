# Application Stack

## NGINX API Gateway

### Location Block Structure

Each client/version binding in an `api-gateway-cm.yaml` takes this
canonical form:

```nginx
# +korio:korioctl:nginx:location:<service-name>
location /api/v1/<client-path>/ {
    set $target http://<service-name>.<subenv>.svc.cluster.local:8080;
    rewrite ^/api/v1/<client-path>/(.*)$ /api/v1/<canonical-path>/$1 break;
    proxy_pass $target;
    auth_request /_auth-via-auth-node;
    auth_request_set $response $upstream_http_user;
    auth_request_set $auth_status $upstream_status;
    proxy_set_header X_HTTP_USER $response;
    proxy_set_header X-Upstream-Service <service-name>;
    add_header X-Debug-Request-Path <service-name>;
}
```

- **Comment annotation** (`# +korio:korioctl:nginx:location:…`) — machine-readable
  marker korioctl uses to identify, generate, and diff blocks when syncing
  internal/external configs.
- **`set $target …`** — points to the *versioned* Kubernetes Service for this
  client (e.g. `sites-node-m4157-p101-phase3`). Using a variable forces NGINX
  to resolve via CoreDNS at request time rather than at startup.
- **`rewrite … break`** — strips the client-specific path prefix before
  forwarding; the microservice only knows its canonical path (e.g.
  `/api/v1/site/`), not the client-specific one.
- **`proxy_pass $target`** — forwards the rewritten request to the versioned
  upstream.
- **`auth_request /_auth-via-auth-node`** — makes a subrequest to the
  internal auth location before proxying; NGINX blocks on 401.
- **`auth_request_set …`** — captures the verified user identity and auth
  status from auth-node's response headers into NGINX variables.
- **`proxy_set_header X_HTTP_USER`** — forwards the verified identity to the
  upstream so the microservice knows who the user is without re-verifying.
- **`X-Upstream-Service` / `X-Debug-Request-Path`** — tag the request and
  response for tracing; captured by the custom log format.

A misconfigured block (wrong service name, wrong rewrite) could silently
route a client to the wrong version of a service — which is why the
`Sync NGINX Config Files` workflow exists and why these files are
generated by korioctl rather than edited by hand.

**`korioPlatformVersion` and the version path suffix**

The version suffix in client-specific location paths (e.g., `-v3.1.0`,
`-m4157-p101-phase3`) is the `korioPlatformVersion` value set in the
ApplicationSet — the same value used as the client/version identifier
throughout the platform. The Helm chart records this value as a pod label.

For services that declare `envFromDownwardAPI` in their AppSet (currently
`int-nest-node` and `int-nest-node-v1-0-0`), the Kubernetes Downward API reads
that label back into the pod as the `KORIO_PLATFORM_VERSION` env var. This
gives the service runtime self-awareness of its own platform version — the same
token NGINX uses in path suffixes to route requests to it. A service can then
use `KORIO_PLATFORM_VERSION` when constructing outbound call paths through
`INTERNAL_BASE_URL` (e.g., `/api/v1/studies-{KORIO_PLATFORM_VERSION}/`),
ensuring that version isolation is preserved across multi-hop internal calls.

`KORIO_APP_NAME` (injected the same way from the `app` pod label) gives the
service its own service name at runtime, useful for logging and
self-identification.

Note: `envFromDownwardAPI` does not control how NGINX routes *incoming*
requests — NGINX's static config drives routing via path prefix matching to
`appName`-derived Service names. `KORIO_PLATFORM_VERSION` is a runtime hint
available to the service itself for constructing outbound call paths.
Int-nest-node does not currently appear in any NGINX location block; its
`envFromDownwardAPI` use is in active development
(`jl/korio-platform-version-env-var/DI-2647`) to enable versioned outbound
routing once the corresponding internal NGINX routes are added.

### End-to-End Request Flow (Login → Client-Specific API Call)

```mermaid
sequenceDiagram
    actor User as Browser
    participant Edge as Cloudflare + AKS Ingress
    participant NX as NGINX (ext. gateway)
    participant AN as auth-node
    participant B2C as Azure B2C
    participant MG as MongoDB
    participant BE as back-end-node
    participant SVC as sites-node-{client}

    rect rgb(220, 235, 255)
        Note over User,B2C: Phase 1 — Login & Client Resolution
        User->>Edge: GET /
        Edge-->>User: portico-react SPA
        User->>B2C: loginRedirect (MSAL)
        B2C-->>User: id_token JWT + redirect → /
        User->>Edge: GET /api/v1/info/ + Bearer id_token
        Edge->>NX: route to ext. gateway
        NX->>AN: auth_request /_auth-via-auth-node
        AN->>B2C: GET JWKS, validate JWT
        AN->>MG: user lookup by emails[0]
        AN-->>NX: 200 + user header
        NX->>BE: GET /api/v1/info/ + X_HTTP_USER
        BE-->>User: client context; SPA navigates → /app-{client}/
    end

    rect rgb(220, 255, 220)
        Note over User,SVC: Phase 2 — Client App Load
        User->>Edge: GET /app-{client}/
        Edge-->>User: app-react-{client} SPA (pinned git SHA, path rewritten → /)
        Note right of User: API paths compiled into JS at build time:<br/>/api/v1/site-{client}/
    end

    rect rgb(255, 245, 210)
        Note over User,SVC: Phase 3 — Client API Call
        User->>Edge: POST /api/v1/site-{client}/ + Bearer id_token
        Edge->>NX: route to ext. gateway
        NX->>AN: auth_request /_auth-via-auth-node
        AN->>B2C: GET JWKS, validate JWT
        AN->>MG: user lookup, assert ACTIVE
        AN-->>NX: 200 + user header
        NX->>SVC: rewrite → /api/v1/site/ + X_HTTP_USER
        SVC-->>User: API response
    end
```

**1. Entry — `portico-react` at `/`**

`portico-react` is deployed at the root path of every sub-environment
(e.g. `https://my.korioclinical.com/`). It is the common entry portal
for all users. Every `app-react-*` variant shares the same
`REACT_APP_REDIRECT_URI` (the bare sub-env domain, no path suffix) and
the same `REACT_APP_AUTH_CLIENT_ID` per sub-env, so after every B2C
auth callback the browser always returns to `portico-react`, regardless
of which app initiated the login. `portico-react` owns token storage
for the whole sub-environment.

**2. Auth — B2C login**

`portico-react` redirects the user to B2C (`REACT_APP_AUTH_AUTHORITY`).
B2C issues a JWT and redirects back to the `REACT_APP_REDIRECT_URI`
(root), landing back at `portico-react`.

**3. Dispatch — `portico-react` routes to the client app**

Holding the token, `portico-react` calls `/api/v1/info/` (→
`back-end-node`) to resolve the user's client context, then redirects
the browser to the client-specific app path (e.g.
`/app-m4157-p101-phase3/`).

**4. Load — client-specific React SPA**

The AKS ingress routes `/app-m4157-p101-phase3` →
`app-react-m4157-p101-phase3` (rewriting to `/`). The user loads a
different React SPA — a specific image pinned to a git commit. The
token from step 2 is already in storage.

**5. API calls — client-specific paths baked into the build**

No env var carries the path prefix. Every client's
`REACT_APP_NODE_API_URL` is identically `https://my.korioclinical.com/api/v1`.
The client-specific path prefix (e.g. `/api/v1/site-m4157-p101-phase3/`)
is **compiled into the JavaScript at the pinned git commit**. This is
intentional: the path is part of the frozen, validated configuration and
cannot drift post-acceptance.

**6. External NGINX — auth + rewrite + proxy**

The external API gateway matches `/api/v1/site-m4157-p101-phase3/`,
subrequests `auth-node` to verify the JWT, rewrites the path to the
canonical `/api/v1/site/`, proxies to
`sites-node-m4157-p101-phase3.my.svc.cluster.local:8080`, and injects
`X_HTTP_USER` with the verified identity.

**7. Service — sees only canonical paths**

The versioned microservice receives a normal `/api/v1/site/` request
with the user identity in a header; it has no knowledge of the
client-specific URL prefix.

**Two backend patterns coexist**

| Client generation | React API path pattern | Routes to |
|---|---|---|
| Legacy (e.g. `m4157-p101-phase3`) | `/api/v1/site-{client}/` | Dedicated `sites-node-{client}` microservice; auth via `auth-node` |
| Newer (e.g. `v3.1.0`, `main`) | `/api/v1/site-{version}/` | `back-end-node-{version}` (monolithic); auth via `back-end-node`; uses the internal NGINX gateway for trusted in-cluster fan-out |

The **internal NGINX gateway** (`internal-api-gateway-cm.yaml`) mirrors
the external one but carries no `auth_request` directives — it is
purely for trusted service-to-service calls originating from
`back-end-node`. `back-end-node` reaches it via its `INTERNAL_BASE_URL`
env var.

## Key Repos and Commands

### korioctl (Go CLI)

Manifest-driven deployment tool that generates ArgoCD AppSet files and
NGINX configs.

```bash
mage build          # Build binary
mage test           # Run tests
mage release <major|minor|patch> [pre]  # Create release
go test ./...       # Run Go tests directly
```

Uses [Mage](https://magefile.org/) as the build tool. Binary is on the
user's PATH.

### dagger-presto (Go + Dagger SDK)

Containerized CI/CD pipeline for manifest-driven deployments.

```bash
dagger call <function-name>   # Run Dagger functions
go build .                    # Build
go test ./...                 # Test
```

CI: `.github/workflows/dagger-entrypoint.yml`

### argocd (YAML)

ArgoCD Application and ApplicationSet definitions per environment in
`apps/`. UIs at `https://argocd-{env}.korioclinical.com` (VPN
required).

### kubernetes-manifests (Helm + Kustomize)

Shared Helm chart at `helm/korio/` used by all application services.
Kustomize overlays in `kustomize/`. **SFTP is deployed exclusively via
Kustomize** (`kustomize/sftp-server/`) — the main Helm chart is not
involved in SFTP at all.

```bash
helm lint helm/korio/
helm template helm/korio/ --values helm/korio/values.yaml
kustomize build kustomize/overlays/dev/configure          # App services
kustomize build kustomize/sftp-server/overlays/prod/accept  # SFTP
```

### presto-besto-manifesto (YAML)

Manifest-driven deployment interface — the primary tool for
controlling what gets deployed where. Developers declare three
attributes per service (container image, env vars, secrets), and
the system generates the hundreds of ArgoCD YAML files needed to
deploy across all environments and sub-environments. Environment
dirs (`dev/`, `test/`, `staging/`, `prod/`, etc.) define what's
deployed where. CI: `.github/workflows/dagger-entrypoint.yml`.

### docker (Docker Compose)

Local dev environment for microservices. Requires sibling repos
(auth-node-llama, users-node-llama, etc.) cloned at the same directory
level.

```bash
./get-env.sh                      # Fetch secrets from Azure Key Vault
docker-compose build               # Build all
docker-compose --profile backend build   # Build backend only
docker-compose up -d               # Start detached
docker-compose down -v             # Stop and wipe volumes
./docker-compose-build-workaround.sh  # Parallel build workaround
```

### korio-nginx-docker (Docker)

Custom Bitnami nginx image with Korio-specific configuration, pushed
to ACR. Adds structured access logging with `InURI`/`OutURI` fields.

```bash
az acr login --name korio
docker buildx build \
  -t korio.azurecr.io/korio-nginx-server:<version> \
  --platform=linux/amd64 --push .
```

## Helm Chart Architecture

The shared Helm chart (`kubernetes-manifests/helm/korio/`) is a single,
generic deployment template used for **every application microservice**
on the platform. It is intentionally service-agnostic: it encodes no
business logic and carries no service-specific defaults. All
customisation is injected at render time via `valuesObject` in each
ArgoCD ApplicationSet — which korioctl generates automatically from the
presto manifests.

**Why one chart for all services?** ~50 services × 9 environments × 5
sub-environments = hundreds of Kubernetes objects. A separate chart per
service would be unscalable to maintain. One chart enforces consistency
(same security model, same probe structure, same secret injection
pattern) while korioctl handles per-service customisation at generation
time.

### Resources the chart can create

All resources except the Deployment are conditional:

| Resource | Created when | Purpose |
|---|---|---|
| `Deployment` | Always | The running workload (container + env + probes + security context) |
| `Service` (ClusterIP) | `service.enabled: true` (default) | Makes pod reachable in-cluster at `{appName}.{namespace}.svc.cluster.local:8080` |
| `Ingress` | `ingress.host != ""` | Routes external HTTPS traffic; uses AKS Web App Routing add-on (`webapprouting.kubernetes.azure.com`) |
| `ServiceAccount` | `workloadIdentityClientId != ""` | Annotates the SA with the Azure Entra app ID, enabling Workload Identity token exchange |
| `ConfigMap` | `env != {}` | Holds non-secret env vars (plus a hardcoded `PORT` entry) |
| `Secret` | `secret != {}` | Inline base64-encoded secrets — rarely used; prefer ExternalSecret |
| `ExternalSecret` | `externalSecret != {}` | Instructs the ExternalSecrets operator to pull named secrets from Azure Key Vault; syncs every 30 s |

### Environment variable pipeline

The Deployment loads env vars from multiple sources, assembled by
korioctl from three inputs (image tag, env vars, secrets) declared in
presto-besto-manifesto:

| `values.yaml` key | Mechanism | Source |
|---|---|---|
| `env` | ConfigMap → `envFrom:configMapRef` | Literal key-value pairs baked into the ArgoCD ApplicationSet |
| `envFrom` | ConfigMap → `env[].valueFrom.configMapKeyRef` | Points to per-subenv `{service}-envfrom` ConfigMaps committed in `argocd/apps/{env}/{subenv}/` |
| `externalSecret` | ExternalSecret → K8s Secret `{appName}-kv` → `envFrom:secretRef` | Maps env var names to Azure Key Vault secret names; fetched at runtime |
| `secret` | K8s Secret → `envFrom:secretRef` | Inline secrets (avoid for sensitive values) |
| `envFromDownwardAPI` | `env[].valueFrom.fieldRef` | Kubernetes Downward API — reads pod metadata fields into env vars; currently used to inject `KORIO_PLATFORM_VERSION` (from the `korioPlatformVersion` pod label set by the Helm chart) and `KORIO_APP_NAME` (from the `app` label); see NGINX section for how `korioPlatformVersion` relates to path routing |

### Checksum-driven rolling restarts

The Deployment template annotates each pod with SHA256 checksums of the
rendered ConfigMap, Secret, and ExternalSecret. When any env var or
secret reference changes, the checksum changes and Kubernetes
automatically triggers a rolling restart — without this, pods would keep
running with stale configuration even after a ConfigMap update.

### Ingress and path rewriting

- **`ingress.rewrite: false`** (default) — path `/` proxies to the
  service unchanged. Used for the common root-path services.
- **`ingress.rewrite: true`** — enables an NGINX regex rewrite. The
  path (e.g. `/app-m4157-p101-phase3`) is stripped before forwarding to
  the pod, so the container only sees `/`. Used for client-specific React
  SPA paths.

TLS always uses the shared `korio-tls` secret provisioned by the AKS
Web App Routing add-on — not managed in this chart.

### How ArgoCD references the chart

Each ApplicationSet generated by korioctl pins the chart source to a
specific git **branch** (not a chart version), one per client/version:

```yaml
source:
  repoURL: https://github.com/korio-clinical/kubernetes-manifests
  targetRevision: release/MP-1   # version-pinned branch
  path: helm/korio
  helm:
    valuesObject:                 # all service config inline here
      appName: users-node-mp201-phase1
      image:
        tag: 1f9cba4c7d...        # git SHA from presto manifest
      env: { ... }
      externalSecret: { ... }
```

The `generators.list` in the ApplicationSet expands one Application per
sub-environment (e.g. configure, accept, my), each deployed into its
own namespace. ArgoCD uses `prune: true` and `selfHeal: true` so the
cluster always converges to what the ApplicationSet declares.

### What Terraform does NOT do

Terraform does **not** deploy application services via this chart.
It deploys ArgoCD itself (via `helm_release "argo_cd"`) and supporting
cluster services (ExternalSecrets operator, Grafana). Application
service deployment is entirely GitOps: commit to `argocd` repo →
ArgoCD auto-sync → Helm render → Kubernetes apply.

### Security context

The chart applies a baseline security context by default (drop ALL
capabilities, disable privilege escalation, seccomp `RuntimeDefault`).
Per-service hardening (non-root UID, read-only root filesystem,
writable `/tmp` emptyDir) is configured via the `deploy.securityContext`
and `deploy.writeableTmpDir` values, which korioctl sets from the
presto manifest's security configuration.

## Microservice Deployment Lifecycle

When a developer adds a feature to a microservice, the sequence is:

**1. Build (microservice repo)**
The developer's CI builds a Docker image and pushes it to ACR
tagged with the git SHA (e.g.,
`korio.azurecr.io/users-node:98960d94d8...`).

**2. Update presto-besto-manifesto** ← _only manual step_
The developer updates `<env>/deployments.yaml` with the new image
SHA, and if needed, updates env vars and/or secrets for that
service. These are the only three things a developer ever needs to
touch. A PR is opened on presto-besto-manifesto.

**3. GitHub Actions → Dagger pipeline**
The `dagger-entrypoint.yml` workflow fires on the PR and calls the
`actions-entrypoint` function in the `dagger-presto` module
(pinned at `v1.0.5`), passing the full GitHub event context.

**4. Dagger → korioctl (PrestoToArgo)**
Dagger clones the presto and argocd repos in memory, detects which
environment directories changed, and calls korioctl's `PushToArgo`
for each.

**5. korioctl generates ArgoCD YAML**
korioctl reads the presto manifests and generates:
- **ApplicationSet YAMLs** — one per service/version/environment,
  covering all sub-environments
- **NGINX ConfigMaps** — mapping URL routes to the correct service
  version

These are committed to the `argocd` repo and a PR is opened
automatically.

**6. ArgoCD PR reviewed and merged** ← _only approval step_
DevOps reviews and merges the auto-generated PR on the `argocd`
repo.

**7. ArgoCD syncs to AKS**
ArgoCD detects the new/updated ApplicationSet, generates
Application objects for each sub-environment, renders the shared
`kubernetes-manifests` Helm chart with the values from the
ApplicationSet (image tag, env vars, secret refs), and applies
the result to AKS.

Everything between steps 3 and 7 is fully automated. The only
human actions required are: updating the image SHA, env vars, and
secrets in presto-besto-manifesto (step 2), and approving the
auto-generated ArgoCD PR (step 6).

## CI Workflows (GitHub Actions)

### argocd

**Test** (`.github/workflows/test.yml`) — Placeholder workflow,
triggered manually via `workflow_dispatch`. Runs a single
`echo Hello, world!` step. Exists to verify Actions are wired up
for the repo.

**Sync NGINX Config Files**
(`.github/workflows/update-internal-api-gw-config.yml`) — Triggered
on any push touching an `api-gateway-cm.yaml` file. Diffs the last
two commits to find changed external API gateway configs, runs
`scripts/generate_internal_nginx_config.sh` to derive the
corresponding internal NGINX config for each, then opens an
auto-generated PR (via `peter-evans/create-pull-request`) to keep
internal and external configs in sync.

### presto-besto-manifesto

**Dagger CICD**
(`.github/workflows/dagger-entrypoint.yml`) — Triggered on PRs
(opened, synchronize, closed) targeting `main` or `test-dagger`.
Assembles a JSON payload from GitHub event context, installs the
Dagger CLI (v0.19.7), then delegates to the `dagger-presto` module
(pinned at `v1.0.5` via SSH) by calling its `actions-entrypoint`
function. Uses a GitHub App token for auth and SSH agent for private
repo access.

## Azure Entra B2C Integration

Scanned from `portico-react-llama` and `auth-node-llama`.

### Tenants (one per environment, shared across all clients)

| Environment | Tenant name | User flow/policy |
|---|---|---|
| dev | `koriodevaad` | `B2C_1_dev_signupsignin` |
| test | `koriotestaad` | `B2C_1_test_signupsignin` |
| staging | `koriostagingaad` | `B2C_1_staging_signupsignin` |
| prod | `korioproductionaad` | `B2C_1_signupsignin` |

Client context is determined post-login (via `back-end-node /api/v1/info/`),
not by separate B2C tenants or policies. The client-specific URL prefix is
compiled into each React SPA at the pinned git commit.

OpenID metadata URL pattern:
```
https://{TENANT_NAME}.b2clogin.com/{TENANT_NAME}.onmicrosoft.com/{POLICY_NAME}/v2.0/.well-known/openid-configuration
```

### Auth flow

```
portico-react (MSAL loginRedirect)
    → Azure B2C (user authenticates)
    → B2C issues ID token, redirects to REACT_APP_REDIRECT_URI (domain root)
    → portico-react stores token in SessionStorage
    → all API calls: Authorization: Bearer {idToken}
    → NGINX auth_request /_auth-via-auth-node
    → auth-node GET /api/v1/auth/verify (Passport-Azure-AD BearerStrategy)
        validates JWT signature via B2C JWKS
        extracts emails[0] → MongoDB user lookup
        asserts user.status == ACTIVE
        sets 'user' response header (JSON-stringified, Turkish chars transliterated)
    → NGINX: proxy_set_header X_HTTP_USER {user header}
    → backend service receives verified user identity
```

**Important:** portico-react sends the **ID token** (not access token) as the
Bearer credential. `acquireTokenSilent()` returns `resp.idToken`.

### Environment variables

**portico-react (frontend)**

| Variable | Purpose |
|---|---|
| `REACT_APP_AUTH_CLIENT_ID` | B2C application client ID |
| `REACT_APP_AUTH_AUTHORITY` | B2C authority URL |
| `REACT_APP_AUTH_KNOWN_AUTHORITY` | Known authority for MSAL validation |
| `REACT_APP_REDIRECT_URI` | Post-auth redirect (bare sub-env domain root) |
| `REACT_APP_SCOPE_1/2/3` | OAuth scopes (defined but functionally fall back to CLIENT_ID scope) |

**auth-node (backend)**

| Variable | Purpose |
|---|---|
| `AZURE_TENANT_ID` | B2C tenant UUID |
| `AZURE_TENANT_NAME` | B2C tenant name (e.g. `korioproductionaad`) |
| `AZURE_CLIENT_ID` | B2C application client ID (also used as token audience) |
| `AZURE_CLIENT_SECRET` | B2C application client secret (for Graph API client credentials flow) |
| `AZURE_POLICY_NAME` | User flow/policy name (e.g. `B2C_1_signupsignin`) |
| `JWT_SECRET_KEY` | Local JWT secret used in test env (`NODE_ENV === 'SQAA'`) only |

### Key source files

| Repo | File | Purpose |
|---|---|---|
| portico-react | `src/config/msal.js` | MSAL config (`msalConfig`, `loginRequest`) |
| portico-react | `src/components/AuthenticationProvider.js` | Redirect-to-login logic |
| portico-react | `src/hooks/api.js` | `acquireTokenSilent()` → ID token → Bearer header |
| auth-node | `src/config/b2c/auth/registery.ts` | `BearerStrategy` configuration |
| auth-node | `src/middlewares/authentication.ts` | `passport.authenticate` middleware |
| auth-node | `src/config/b2c/management/graph.ts` | Microsoft Graph API calls |
| auth-node | `src/config/b2c/management/authProvider.ts` | Graph API MSAL client credentials |

### Libraries

| Repo | Library | Version |
|---|---|---|
| portico-react | `@azure/msal-browser` | ^3.7.0 |
| portico-react | `@azure/msal-react` | ^2.0.3 |
| auth-node | `passport-azure-ad` | ^4.3.4 |
| auth-node | `@azure/msal-node` | ^1.15.0 |
| auth-node | `@microsoft/microsoft-graph-client` | ^3.0.5 |

### User lifecycle (Microsoft Graph API)

auth-node manages users in B2C via the Graph API using the **client credentials
flow** (`AZURE_CLIENT_ID` + `AZURE_CLIENT_SECRET`), scope
`https://graph.microsoft.com/.default` (requires admin consent).

| Operation | Graph endpoint |
|---|---|
| Create user | `POST /users` |
| Find user | `GET /users?$filter=startswith(userPrincipalName,...)` |
| Reset password | `PATCH /users/{oid}` → `passwordProfile` |
| Enable / disable | `PATCH /users/{oid}` → `accountEnabled` |

User principal name format: `{email_@_replaced_with_underscore}#EXT#@{TENANT_NAME}.onmicrosoft.com`

The B2C object ID is stored in Korio's MongoDB as `user.azure_oid`.

### Session and security details

- Token stored in **SessionStorage** (cleared on tab close); not in cookies
- 15-minute inactivity timeout tracked via `localStorage.korioActivityTimestamp`
- `validateIssuer: false` in BearerStrategy — intentional for B2C multi-policy setups
- Token claims used: `emails[0]` (user lookup key), `iat` (audit trail)
- Test environment (`NODE_ENV === 'SQAA'`) bypasses B2C and uses local JWT validation
